{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP IA Developpementale\n",
    "## Alexis Pister - Raphael Teitgen\n",
    "\n",
    "Le but de ce TD est de développer un agent qui présente un début de comportement d'IA développemental. Il sera modelisé au sein d'un environnement et se constituera une représentation de celui-ci au fur et à mesure à partir des intéractions qu'il entretiendra avec celui-ci. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1\n",
    "\n",
    "Dans un premier temps, l'agent est modelisé dans un environnement parmis 2 possibles, et peut exercer une action $a = \\{a_1, a_2\\}$ sur celui-ci. Un feedback $\\{f_1, f_2\\}$ est alors retourné par l'environnement selon l'action. Cependant, le feedback retourné n'est pas le même selon l'environnement pour une action donnée. Notre but est que l'agent anticipe le feedback qu'il va recevoir par l'environnement à partir d'une action donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pas content\n",
      "pas content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "TRACES :\n",
      "Actions exectutées :  [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
      "Feedbacks recus :  [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
      "Anticipations :  [None, None, 0, 1, 1, 0, 1, 1, 0, 0]\n",
      "Satisfactions :  ['pas content', 'pas content', 'content', 'content', 'content', 'content', 'content', 'content', 'content', 'content']\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.environnement = env\n",
    "        self.dic = {0 : [], 1 : []} # Feedbacks received from actions\n",
    "        self.ACTIONS = [] # Trace of actions\n",
    "        self.ANTICIPATIONS = [] # Trace of anticipations\n",
    "        self.S = [] # Trace of Satisfactions\n",
    "        self.F = [] # Trace of Feedbacks\n",
    "        \n",
    "        \n",
    "    def run(self, k):\n",
    "        i = 0\n",
    "        while i < k:\n",
    "            # Choose the action randomly\n",
    "            choix = randint(0,1)\n",
    "            self.ACTIONS.append(choix)\n",
    "            \n",
    "            if self.dic[choix] == []:\n",
    "                # If it is the first encounter of this actions, does not have any anticipation\n",
    "                anticipation = None\n",
    "            else:\n",
    "                # Get the element of maximum occurence as anticipation\n",
    "                anticipation = max(self.dic[choix],key=self.dic[choix].count)    \n",
    "            self.ANTICIPATIONS.append(anticipation)\n",
    "            \n",
    "            # Get the feedback\n",
    "            feedback = self.environnement.action(choix)\n",
    "            # Append on the trace\n",
    "            self.dic[choix].append(feedback)\n",
    "            self.F.append(feedback)\n",
    "            \n",
    "            # If he's got the good prediction he is happy\n",
    "            if anticipation == feedback:\n",
    "                print('content')\n",
    "                self.S.append('content')\n",
    "            else:\n",
    "                print('pas content')\n",
    "                self.S.append('pas content')\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    def retour(self):\n",
    "        print('TRACES :')\n",
    "        print('Actions exectutées : ', self.ACTIONS)\n",
    "        print('Feedbacks recus : ', self.F)\n",
    "        print('Anticipations : ', self.ANTICIPATIONS)\n",
    "        print('Satisfactions : ', self.S)\n",
    "        \n",
    "\n",
    "            \n",
    "class Environnement:\n",
    "    def __init__(self,val):\n",
    "        if val == 0:\n",
    "            self.f1 = 0\n",
    "            self.f2 = 1\n",
    "        elif val == 1:\n",
    "            self.f1 = 1\n",
    "            self.f2 = 0\n",
    "    def action(self,action):\n",
    "        if action == 0:\n",
    "            return self.f1\n",
    "        elif action == 1:\n",
    "            return self.f2\n",
    "        \n",
    "\n",
    "env = Environnement(randint(0,1))\n",
    "Agent = Agent(env)\n",
    "Agent.run(10)\n",
    "Agent.retour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour résoudre ce problème nous avons implementé une classe Environement comprenant en attributs $(f_1,f_2)$ initialisé à $(0,1)$ ou $(1,0)$ par le constructeur. Sa méthode action retourne le feedback $f_i$ pour l'action $a_i \\in \\{0,1\\}$ donné en argument.  \n",
    "La classe Agent possède en attribut l'environnement dans lequel il est modelisé, ainsi qu'un dictionnaire self.dic ayant comme clés les actions possibles pour l'agent et en valeur des listes sauvegardant tous les feedbacks reçus au cours du temps pour tel action. L'agent conserve en trace tous les actions, feedbacks, anticipations et satisfactions rencontrés sous forme de listes.\n",
    "\n",
    "La méthode run s'execute ainsi $k$ fois :\n",
    "* L'agent choisi son action au hasard\n",
    "* Il prédit le feedback comme étant le feedback reçu le plus de fois pour cette action. Si c'est la premiere fois qu'il exectue cette action, il n'a pas de prédiction\n",
    "* Il reçoit le feedback par l'environnement\n",
    "* Si la prédiction est juste, l'agent est 'content', sinon il n'est 'pas content'\n",
    "\n",
    "On peut voir que quelque soit l'environnement choisi, l'agent est toujours 'pas content' la premiere fois qu'il utilise une nouvelle action, puis prédit ensuite toujours le bon feedback pour cette action, ce qui est logique étant donné que les feedbacks restent les mêmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2\n",
    "Nous allons désormais améliorer notre agent en lui rajoutant un systeme de valeur de la forme $\\{ (a_i, f_i) : v_{ii} \\}$. Les intéraction de l'agent avec son environnement auront désormais un poids représentant la satisfaction de l'agent et notre objectif est que l'il choisisse des actions lui renvoyant des valeurs de satisfacton le plus important possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content\n",
      "pas content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "TRACES :\n",
      "Actions exectutées :  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Feedbacks recus :  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Anticipations :  [None, None, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Satisfactions :  [1, -1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{0: [0, 0, 0, 0, 0, 0, 0, 0, 0], 1: [1]}\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, systeme_valeur):\n",
    "        self.environnement = env\n",
    "        \n",
    "        self.dic = {0 : [], 1 : []} # Feedbacks received from actions\n",
    "        self.ACTIONS = [] # Trace of actions\n",
    "        self.ANTICIPATIONS = [] # Trace of anticipations\n",
    "        self.S = [] # Trace of Satisfactions\n",
    "        self.F = [] # Trace of Feedbacks\n",
    "        \n",
    "        # Systeme de valeurs de l'agent\n",
    "        self.valeurs = systeme_valeur\n",
    "        \n",
    "        \n",
    "    def run(self, k):\n",
    "        # compteur des actions\n",
    "        a = 0\n",
    "        # compteur de la boucle\n",
    "        i = 0\n",
    "        while i < k:\n",
    "            if a < len(self.dic.keys()):\n",
    "                # Execute toutes les actions possibles une fois au début de la simulation\n",
    "                # Pas d'anticipation possible pour l'instant\n",
    "                anticipation = None\n",
    "                choix = a\n",
    "                a += 1\n",
    "            else:\n",
    "                # Parcours toutes les actions possible, fais une prédiction du feedback comme ci-dessus, et choisi\n",
    "                # l'action qui a l'interaction (a, fprediction) retournant la plus grande valeur a partir du\n",
    "                # systeme de valeur de l'agent\n",
    "                choix = -10000000\n",
    "                for action in self.dic.keys():\n",
    "                    prediction = max(self.dic[action],key=self.dic[action].count)\n",
    "                    if self.valeurs.valeurs[(action, prediction)] > choix:\n",
    "                        choix = action\n",
    "                        anticipation = prediction\n",
    "            \n",
    "            self.ACTIONS.append(choix)\n",
    "            self.ANTICIPATIONS.append(anticipation)\n",
    "            \n",
    "            feedback = self.environnement.action(choix)\n",
    "            self.dic[choix].append(feedback)\n",
    "            self.F.append(feedback)\n",
    "            \n",
    "            # L'agent est content si la valeur motivationnelle est supérieure à 0\n",
    "            if self.valeurs.valeurs[(choix, feedback)] > 0:\n",
    "                print('content')\n",
    "            else:\n",
    "                print('pas content')\n",
    "            self.S.append(self.valeurs.valeurs[(choix, feedback)])\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    def retour(self):\n",
    "        print('TRACES :')\n",
    "        print('Actions exectutées : ', self.ACTIONS)\n",
    "        print('Feedbacks recus : ', self.F)\n",
    "        print('Anticipations : ', self.ANTICIPATIONS)\n",
    "        print('Satisfactions : ', self.S)\n",
    "        \n",
    "\n",
    "class Valeur:\n",
    "    def __init__(self, Type):\n",
    "        # Systeme de valeur sous forme de dictionnaire {(action, feedback) : valeur}\n",
    "        if Type == 0:\n",
    "            self.valeurs = {(0,0) : 1, (0,1) : 1, (1, 0) : -1, (1, 1) : -1}\n",
    "        elif Type == 1:\n",
    "            self.valeurs = {(0,0) : -1, (0,1) : -1, (1, 0) : 1, (1, 1) : 1}\n",
    "        elif Type == 2:\n",
    "            self.valeurs = {(0,0) : -1, (0,1) : 1, (1, 0) : -1, (1, 1) : 1}\n",
    "        \n",
    "    def valeur(self, action, feedback):\n",
    "        #Retourne la valeur associé a l'interaction donnée en entree\n",
    "        return self.valeurs[(action, feedback)]\n",
    "        \n",
    "            \n",
    "class Environnement:\n",
    "    def __init__(self,val):\n",
    "        if val == 0:\n",
    "            self.f1 = 0\n",
    "            self.f2 = 1\n",
    "        elif val == 1:\n",
    "            self.f1 = 1\n",
    "            self.f2 = 0\n",
    "            \n",
    "    def action(self,action):\n",
    "        #Retourne le feedback en fonction de l'action\n",
    "        if action == 0:\n",
    "            return self.f1\n",
    "        elif action == 1:\n",
    "            return self.f2\n",
    "        \n",
    "\n",
    "env = Environnement(randint(0,1))\n",
    "Agent = Agent(env, Valeur(0))\n",
    "Agent.run(10)\n",
    "Agent.retour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc implementé une classe Valeur définissant le système de valeur sous dictionnaire et que l'agent possède comme attribut. La méthode run() a été modifié pour que l'agent prenne en compte son système de valeurs et fonctionne désormait ainsi :\n",
    "* L'agent parcours ses actions possibles et fait une prédiction sur le feedback associé comme précédent\n",
    "* Il choisi l'action $a_i$ avec le couple $(a_i, f_{prediction})$ ayant la plus grande valeur associé avec son systeme de valeur\n",
    "* Il reçoit le feedback par l'environnement\n",
    "* Si la valeur associé à l'interaction réalisée est positive, l'agent est 'content', sinon il n'est 'pas content'  \n",
    "\n",
    "Au début de la modélisation, l'agent exerce toutes ses actions possible une fois pour pouvoir faire des prédictions du feedback.\n",
    "\n",
    "On peut voir dans la trace rendu qu'une fois toutes les actions executées une fois, l'agent n'utilise plus que action ayant une intéraction qui retourne à l'agent la valeur maximale de son systeme de valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant modifier légèrement notre code pour modéliser un environnement 3 qui agit comme l'environnement 1 les 5 premières itérations puis comme l'environnement 2 ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pas content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "pas content\n",
      "pas content\n",
      "pas content\n",
      "pas content\n",
      "pas content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "content\n",
      "TRACES :\n",
      "Actions exectutées :  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "Feedbacks recus :  [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "Anticipations :  [None, None, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "['Non prédit', 'Non prédit', 'Predit', 'Predit', 'Predit', 'Non prédit', 'Non prédit', 'Non prédit', 'Non prédit', 'Non prédit', 'Non prédit', 'Non prédit', 'Predit', 'Predit', 'Predit']\n",
      "Satisfactions :  [-1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "class Environnement:\n",
    "    def __init__(self,val):\n",
    "        if val == 0:\n",
    "            self.f1 = 0\n",
    "            self.f2 = 1\n",
    "        elif val == 1:\n",
    "            self.f1 = 1\n",
    "            self.f2 = 0\n",
    "            \n",
    "    def action(self,action):\n",
    "        #Retourne le feedback en fonction de l'action\n",
    "        if action == 0:\n",
    "            return self.f1\n",
    "        elif action == 1:\n",
    "            return self.f2\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, systeme_valeur):\n",
    "        if env == 2:\n",
    "            self.environnement = Environnement(0)\n",
    "            self.env3 = True\n",
    "        else:\n",
    "            self.environnement = Environnement(env)\n",
    "            self.env3 = False\n",
    "        \n",
    "        # Dictionnaire action : feedbacks recus au fil du temps\n",
    "        self.dic = {0 : [], 1 : []}\n",
    "        self.ACTIONS = []\n",
    "        self.ANTICIPATIONS = []\n",
    "        self.GOOD_PRED = []\n",
    "        self.S = [] #Satisfactions\n",
    "        self.F = [] #Feedbacks\n",
    "        \n",
    "        # Systeme de valeurs de l'agent\n",
    "        self.valeurs = systeme_valeur\n",
    "        \n",
    "        \n",
    "    def run(self, k):\n",
    "        # compteur des actions\n",
    "        a = 0\n",
    "        # compteur de la boucle\n",
    "        i = 0\n",
    "        while i < k:\n",
    "            # Changement d'environnement pour l'environnement 3\n",
    "            if (self.env3 == True) and (i == 5):\n",
    "                self.environnement = Environnement(1)\n",
    "            \n",
    "            if a < len(self.dic.keys()):\n",
    "                # Execute toutes les actions possibles une fois au début de la simulation\n",
    "                # Pas d'anticipation possible pour l'instant\n",
    "                anticipation = None\n",
    "                choix = a\n",
    "                a += 1\n",
    "            else:\n",
    "                # Parcours toutes les actions possible, fais une prédiction du feedback comme ci-dessus, et choisi\n",
    "                # l'action qui a l'interaction (a, fprediction) retournant la plus grande valeur a partir du\n",
    "                # systeme de valeur de l'agent\n",
    "                choix = -10000000\n",
    "                for action in self.dic.keys():\n",
    "                    prediction = max(self.dic[action],key=self.dic[action].count)\n",
    "                    if self.valeurs.valeurs[(action, prediction)] > choix:\n",
    "                        choix = action\n",
    "                        anticipation = prediction\n",
    "            \n",
    "            self.ACTIONS.append(choix)\n",
    "            self.ANTICIPATIONS.append(anticipation)\n",
    "            \n",
    "            feedback = self.environnement.action(choix)\n",
    "            self.dic[choix].append(feedback)\n",
    "            self.F.append(feedback)\n",
    "            \n",
    "            # Bonne prediction du feedback ?\n",
    "            if anticipation == feedback:\n",
    "                self.GOOD_PRED.append('Predit')\n",
    "            else:\n",
    "                self.GOOD_PRED.append('Non prédit')\n",
    "            \n",
    "            # L'agent est content si la valeur motivationnelle est supérieure à 0\n",
    "            if self.valeurs.valeurs[(choix, feedback)] > 0:\n",
    "                print('content')\n",
    "            else:\n",
    "                print('pas content')\n",
    "            self.S.append(self.valeurs.valeurs[(choix, feedback)])\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    def retour(self):\n",
    "        print('TRACES :')\n",
    "        print('Actions exectutées : ', self.ACTIONS)\n",
    "        print('Feedbacks recus : ', self.F)\n",
    "        print('Anticipations : ', self.ANTICIPATIONS)\n",
    "        print(self.GOOD_PRED)\n",
    "        print('Satisfactions : ', self.S)\n",
    "        \n",
    "\n",
    "class Valeur:\n",
    "    def __init__(self, Type):\n",
    "        # Systeme de valeur sous forme de dictionnaire {(action, feedback) : valeur}\n",
    "        if Type == 0:\n",
    "            self.valeurs = {(0,0) : 1, (0,1) : 1, (1, 0) : -1, (1, 1) : -1}\n",
    "        elif Type == 1:\n",
    "            self.valeurs = {(0,0) : -1, (0,1) : -1, (1, 0) : 1, (1, 1) : 1}\n",
    "        elif Type == 2:\n",
    "            self.valeurs = {(0,0) : -1, (0,1) : 1, (1, 0) : -1, (1, 1) : 1}\n",
    "        \n",
    "    def valeur(self, action, feedback):\n",
    "        #Retourne la valeur associé a l'interaction donnée en entree\n",
    "        return self.valeurs[(action, feedback)]\n",
    "        \n",
    "\n",
    "#0 1 ou 2\n",
    "env = 2\n",
    "#0 1 ou 2\n",
    "vals = 2\n",
    "\n",
    "systeme_valeur = Valeur(vals)\n",
    "Agent = Agent(env, systeme_valeur)\n",
    "Agent.run(15)\n",
    "Agent.retour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelque soit le système de valeur, on remarque qu'à partir du 6ème tour quand l'environnement change l'agent va se tromper sur son anticipation du feedback 5 fois avant de refaire des bonnes prédictions à nouveaux. Cela est logique, avec cette implémentation l'agent fait comme prédiction le feedback qu'il a reçu le plus de fois dans sa vie pour une action donnée, et il faut donc qu'il reçoit le nouveau feedback du nouvel environnement plus de fois que l'ancien (ici 5 fois pour l'action choisie qui lui ramène le plus de satisfaction) pour ensuite refaire une bonne prédiction. \n",
    "\n",
    "Pour les systèmes de valeurs 1 et 2, on observe que même si l'agent se trompe sur la prédiction au bout du 6ème pas de temps il est toujours satisfait. En effet dans ces 2 systèmes de valeurs, la satisfaction est directement correlée à l'action. Dans le 1er systeme de valeur par exemple l'action 0 donnera toujours une satisfaction de 1 quelque soit le feedback. Même si le feedbakc change l'agent continuera donc a faire la même action qui lui donne le plus de satisfaction.\n",
    "\n",
    "Cependant, pour le 3ème système de valeur, a partir du moment où l'environnement change l'intéraction qui donnait une satisfaction positive en donne désormais une négative. Une fois que l'agent refait des bonnes prédictions à partir de son apprentissage, il choisira de nouveau la meilleurs intéraction qui lui donne le plus de satisfacton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 3\n",
    "\n",
    "Nous allons désormais implémenter un agent qui essaye d'avoir un mémoire de ses intéraction et de sélectionner ses actions par rapport aux intéractions qu'il a vécu aux tours précédants. pour commencer, l'agent va se baser seulement sur l'intéraction du tour précédent pour ses choix.\n",
    "Un Environnement 4 va être implementé pour tester cet ajout. Celui ci retourne comme feedback $f_2$ seulement si l'agent execute une action différente de celle actionnée au temps précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACES :\n",
      "Actions exectutées :  [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Feedbacks recus :     [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Anticipations :       [None, 1, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['Non prédit', 'Non prédit', 'Non prédit', 'Predit', 'Non prédit', 'Predit', 'Non prédit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Non prédit', 'Non prédit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit', 'Predit']\n",
      "Satisfactions :       [1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Memoire :             {((0, 1), (1, 1)): 2, ((0, 0), (1, 1)): 0, ((1, 0), (0, 1)): 0, ((0, 1), (0, 0)): 0, ((1, 0), (1, 0)): -2, ((1, 1), (1, 0)): 0, ((1, 1), (0, 1)): 2}\n"
     ]
    }
   ],
   "source": [
    "class Environnement:\n",
    "    def __init__(self,val):\n",
    "        self.env = val\n",
    "        if val == 0:\n",
    "            self.f1 = 0\n",
    "            self.f2 = 1\n",
    "        elif val == 1:\n",
    "            self.f1 = 1\n",
    "            self.f2 = 0\n",
    "        self.action_precedante = None\n",
    "            \n",
    "    def action(self,action):\n",
    "        #Retourne le feedback en fonction de l'action\n",
    "        if self.env == 3:\n",
    "            # Environnement 4 : retourne f2 si l'agent change d'action et f1 sinon\n",
    "            if action == self.action_precedante:\n",
    "                self.action_precedante = action\n",
    "                return 0\n",
    "            else:\n",
    "                self.action_precedante = action\n",
    "                return 1\n",
    "        else:\n",
    "            if action == 0:\n",
    "                return self.f1\n",
    "            elif action == 1:\n",
    "                return self.f2\n",
    "        \n",
    "        \n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, systeme_valeur):\n",
    "        if env == 2:\n",
    "            self.environnement = Environnement(0)\n",
    "            self.env3 = True\n",
    "        else:\n",
    "            self.environnement = Environnement(env)\n",
    "            self.env3 = False\n",
    "        \n",
    "        # Dictionnaire action : feedbacks recus au fil du temps\n",
    "        self.dic = {0 : [], 1 : []}\n",
    "        self.ACTIONS = []\n",
    "        self.ANTICIPATIONS = []\n",
    "        self.GOOD_PRED = []\n",
    "        self.S = [] #Satisfactions\n",
    "        self.F = [] #Feedbacks\n",
    "        \n",
    "        # Systeme de valeurs de l'agent\n",
    "        self.valeurs = systeme_valeur\n",
    "        \n",
    "        # Satisfactions associés a chaque tuple d'interaction possible\n",
    "        self.memoire = {}\n",
    "        \n",
    "        \n",
    "    def run(self, k, expl=10):\n",
    "        # compteur des actions\n",
    "        a = 0\n",
    "        # compteur de la boucle\n",
    "        i = 0\n",
    "        while i < k:\n",
    "            # Changement d'environnement pour l'environnement 3\n",
    "            if (self.env3 == True) and (i == 5):\n",
    "                self.environnement = Environnement(1)\n",
    "            \n",
    "            if i < expl:\n",
    "                # Execute ses actions au hasard au début (exploration)\n",
    "                choix = randint(0,1)\n",
    "                if self.dic[choix] == []:\n",
    "                    # If it is the first encounter of this actions, does not have any anticipation\n",
    "                    anticipation = None\n",
    "                else:\n",
    "                    # Get the element of maximum occurence as anticipation\n",
    "                    anticipation = max(self.dic[choix],key=self.dic[choix].count)\n",
    "            else:\n",
    "                # A partir de l'interaction du tour précédent, l'agent choisit l'action \n",
    "                sat = -10\n",
    "                for mem in self.memoire.keys():\n",
    "                    if interaction_precedante == mem[0]:\n",
    "                        if sat < self.memoire[mem]:\n",
    "                            sat = self.memoire[mem]\n",
    "                            choix = mem[1][0]\n",
    "                \n",
    "#                 print(self.memoire)\n",
    "#                 print(choix)\n",
    "                \n",
    "                anticipation = max(self.dic[choix],key=self.dic[choix].count)\n",
    "            \n",
    "            # Get feedback\n",
    "            feedback = self.environnement.action(choix)\n",
    "            self.dic[choix].append(feedback)\n",
    "            \n",
    "            # Save trace\n",
    "            self.ACTIONS.append(choix)\n",
    "            self.ANTICIPATIONS.append(anticipation)\n",
    "            self.S.append(self.valeurs.valeurs[(choix, feedback)])\n",
    "            self.F.append(feedback)\n",
    "            \n",
    "            # Bonne prediction du feedback ?\n",
    "            if anticipation == feedback:\n",
    "                self.GOOD_PRED.append('Predit')\n",
    "            else:\n",
    "                self.GOOD_PRED.append('Non prédit')\n",
    "            \n",
    "            if len(self.S) > 1:\n",
    "                # On rajoute dans le dictionnaire la satisfaction associé à l'interaction (t-1)+(t)\n",
    "                satisfaction2 = self.S[-1] + self.S[-2]\n",
    "                self.memoire[(interaction_precedante, (choix, feedback))] = satisfaction2\n",
    "            \n",
    "            # Sauvegarde de l'interaction pour le tour suivant\n",
    "            interaction_precedante = (choix, feedback)\n",
    "            i += 1\n",
    "            \n",
    "    def retour(self):\n",
    "        print('TRACES :')\n",
    "        print('Actions exectutées : ', self.ACTIONS)\n",
    "        print('Feedbacks recus :    ', self.F)\n",
    "        print('Anticipations :      ', self.ANTICIPATIONS)\n",
    "        print(self.GOOD_PRED)\n",
    "        print('Satisfactions :      ', self.S)\n",
    "        print('Memoire :            ', self.memoire)\n",
    "        \n",
    "\n",
    "class Valeur:\n",
    "    def __init__(self, Type):\n",
    "        # Systeme de valeur sous forme de dictionnaire {(action, feedback) : valeur}\n",
    "        if Type == 0:\n",
    "            self.valeurs = {(0,0) : 1, (0,1) : 1, (1, 0) : -1, (1, 1) : -1}\n",
    "        elif Type == 1:\n",
    "            self.valeurs = {(0,0) : -1, (0,1) : -1, (1, 0) : 1, (1, 1) : 1}\n",
    "        elif Type == 2:\n",
    "            self.valeurs = {(0,0) : -1, (0,1) : 1, (1, 0) : -1, (1, 1) : 1}\n",
    "        \n",
    "    def valeur(self, action, feedback):\n",
    "        #Retourne la valeur associé a l'interaction donnée en entree\n",
    "        return self.valeurs[(action, feedback)]\n",
    "        \n",
    "\n",
    "#0 1 2 ou 3 (env4)\n",
    "env = 3\n",
    "#0 1 ou 2\n",
    "vals = 2\n",
    "\n",
    "systeme_valeur = Valeur(vals)\n",
    "Agent = Agent(env, systeme_valeur)\n",
    "Agent.run(30, expl= 15)\n",
    "Agent.retour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour répondre à ce probème, nous avons ajouté un dictionnaire qui représente la mémoire de l'agent. Celui-ci contient en clés des couples d'intéractions sur 2 pas de temps et en valeur la somme des satisfactions des 2 intéractions réalisées à la suite. Quand l'agent effectue une nouvelle intéraction, il met à jour sa mémoire pour les 2 dernière intéractions vécues.\n",
    "\n",
    "Pour 4 intéractions possibles comme dans notre simulation, 16 tuples sont possibles. L'agent va donc au début être dans une phase d'exploration ou il choisira ses actions au hasard, mettant à jour au fur et à mesure son dictionnaire.\n",
    "\n",
    "Pour les 2 premier systèmes de valeurs, la satisfaction reste la même par rapport à une action donnée. L'agent va donc trouver l'action qui lui rapporte la satisfaction maximale dans la phase d'exploration puis constamment executer celle-ci ensuite.\n",
    "\n",
    "Pour le 3eme système de valeur dans l'environnement 4, l'agent devra alterner ses 2 actions pour maximiser son gain de satisfaction. Avec un nombre de pas d'exploration assez important, l'agent apprend bien que c'est en alternant ses actions qu'il obtient le plus de points grâce à sa mémoire à 2 pas. Il alterne alors ses actions en continue après cette phase terminée."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
